---
#!./large-source-code-probability-distributions.knit.sh
title: "Generating Random Code that Looks Real"
output:
  html_notebook: default
date: '2020-12-?? ??:??:29 +0000'
layout: post
categories:
  - Research
tags:
  - Smalltalk
  - Pharo
  - Language Implementation
  - Research
  - SOMns
  - Metrics
custom_scss: |
  figure { margin-left: 0; margin-right: 0; }
  figure img { margin-left: auto; margin-right: auto; display: block; }
  figure.full img { max-width: 100%; }
  figure.two-thirds img { max-width: 66%; }

htmlmeta:
  description: TODO
  keywords: Smalltalk, Pharo, Language Implementation, Research, SOMns, Metrics
  author: Stefan Marr
---

In an attempt to have better benchmarks for virtual machine, interpreter,
and compiler research, I am currently trying to generate code that is in some
ways realistic, but easier to work with than actual applications.

In the last two blog posts, I looked at static metrics for code to get an idea
of how large code bases actually look like.

In this post, I am setting out to use what I learned about the typical length
of methods and generate millions of lines of code that look like they could
actually be real. At least when only considering the length of methods.

Small steps...

So, let's get started by looking at how the length of methods look like in
large code bases.

Before we get started, just one more simplification: I will only consider
methods that have 1 to 30 lines (of code).
Setting an upper bound will make some of the steps here simpler.
And a little silly, but nonetheless an issue, it will avoid me having to change
one of the language implementations I am interested in, which is unfortunately
limited to 128 bytecodes and 128 literals (constants, method names, etc),
which in practice translates to somethine like 30 lines of code.
While this could be fixed, let's assume 30 lines of code per method
ought to be enough for anybody...



```{r setup, echo=FALSE, include=FALSE, cache=FALSE}
if (!suppressPackageStartupMessages(library(here, logical.return=TRUE))) {
  install.packages("here", repos="https://cloud.r-project.org/")
  library(here)
}
source(here("libs", "common.R"), chdir = TRUE)
#install.packages("psych")
library(psych)


knitr::opts_chunk$set(
  echo = FALSE,
  dev = "ragg_png",
  dpi = 192) #  dpi = 192

knitr::knit_hooks$set(
  plot = function(x, options) {
    cap  <- options$fig.cap  # figure caption
    tags <- htmltools::tags
    as.character(tags$img(src = x, alt = cap))
  }
)

# install.packages("ParetoPosStable")
library(ParetoPosStable)

base_dir <- '/Users/smarr/Projects/Smalltalk-Code-Characteristics/data-2020-12/'

method_files <- c(
  './part-2/internal/methods.csv',
  './part-1/internal/methods.csv'
)

load_all <- function (file_names) {
  file_names %>%
    map(function (file_name) {
      cat(file_name, " ")
      read.csv(paste0(base_dir, file_name))
    }) %>%
    reduce(rbind)
}

methods_data <- load_all(method_files) %>%
  select(-c(X, id)) %>%
  distinct(class, selector, .keep_all = TRUE)

methods_data_ruby <- read.csv("/Users/smarr/Projects/Smalltalk-Code-Characteristics/Ruby-data/methods.csv")
```


## Length of Methods

When it comes to measuring the length of methods,
there are plenty of possibly ways to go about.
Pharo counts the non-empty lines.
And for Ruby, I counted either all lines, or the lines that are not just empty
and not just comments.

The histogram below shows the results for methods with 1-30 lines.


```{r loc-per-methods, results='asis', cache=TRUE, fig.height=2.6, fig.width=8}
loc_per_method_pharo <- methods_data %>%
    rename(num = linesOfCode)

loc_per_method_ruby <- methods_data_ruby %>%
    rename(num = linesOfCode)

lines_per_method_ruby <- methods_data_ruby %>%
    rename(num = lines)

# scale_color_manual(values = c("#fcaf3e", "#3465a4", "#ad7fa8")) +

cp("<figure class='full'>")
grid.arrange(
  num_hist(loc_per_method_pharo  %>% filter(num <= 30), 1, "#3465a4", "number of methods", "non-empty lines (Pharo)"),
  num_hist(loc_per_method_ruby   %>% filter(num <= 30), 1, "#fcaf3e", NULL, "lines of code (Ruby)"),
  num_hist(lines_per_method_ruby %>% filter(num <= 30), 1, "#ad7fa8", NULL, "lines (Ruby)") +
    theme(plot.margin = unit(c(5,5,0,0), "mm")),
  ncol = 3)
cp("<figcaption>Distribution of the length of methods.</figcaption>")
cp("</figure>")
```

Despite difference in languages and metrics, we see a pretty similar shape.
Perhaps with the exception of the methods with 1-3 lines.

## Generating Realistic Method Length from Uniform Random Numbers

Before actually generating method length randomly, let's define the goal a bit
more clearly.

In the end, I do want to be able to generate a code base where
the length of methods has a distribution very similar to what we see
for Ruby and Pharo.

Though, the random number generators we have in most systems generate numbers
in a uniform distribution typically in the range from 0 to 1.
This means, each number between 0 and 1 is going to be equally likely to be
picked.
To get other kinds of distributions, for instance,
the normal distribution, we can use what is called the *inverted cumulative distribution function*.
When we throw our uniformly distributed numbers into this function,
we should end up with random numbers that are distrubted according to the
distribution that we want.

This means, I need to do the following:

1. determine the cumulative distribution of the method length
2. approximate a function to represent the cumulative distribution
3. invert the function


### Determining the Cumulative Distribution

Ok, so, the first step is to determine the cumulative distribution.


```{r}
loc_freq_pharo <- methods_data %>%
  filter(linesOfCode <= 30) %>%
  group_by(linesOfCode) %>%
  summarise(
    num = n(),
    type = "Pharo",
    .groups = "drop") %>%
  rename(length = linesOfCode)
loc_freq_pharo$cum <- cumsum(loc_freq_pharo$num)
loc_freq_pharo$cum_p <- loc_freq_pharo$cum / max(loc_freq_pharo$cum)

loc_freq_ruby <- methods_data_ruby %>%
  filter(linesOfCode <= 30) %>%
  group_by(linesOfCode) %>%
  summarise(
    num = n(),
    type = "Ruby (LOC)",
    .groups = "drop") %>%
  rename(length = linesOfCode)
loc_freq_ruby$cum <- cumsum(loc_freq_ruby$num)
loc_freq_ruby$cum_p <- loc_freq_ruby$cum / max(loc_freq_ruby$cum)

line_freq_ruby <- methods_data_ruby %>%
  filter(lines <= 30) %>%
  group_by(lines) %>%
  summarise(
    num = n(),
    type = "Ruby (lines)",
    .groups = "drop") %>%
  rename(length = lines)
line_freq_ruby$cum <- cumsum(line_freq_ruby$num)
line_freq_ruby$cum_p <- line_freq_ruby$cum / max(line_freq_ruby$cum)

loc_freq <- rbind(loc_freq_pharo, loc_freq_ruby, line_freq_ruby)


loc_freq_means <- loc_freq %>%
  filter(length > 0) %>%
  group_by(length) %>%
  summarise(mean = mean(cum_p),
            geo.mean = geometric.mean(cum_p),
            harm.mean = harmonic.mean(cum_p),
            .groups = "drop") %>%
  pivot_longer(ends_with("mean"), names_to = "type", values_to = "cum_p") %>%
  transform(cum = NA, num = NA)

install.packages("gghighlight")
library(gghighlight)

ggplot(rbind(loc_freq, loc_freq_means) %>% filter(length > 0),
       aes(x = length, y = cum_p * 100, color = type)) +
  geom_point(size = 0.7, alpha = 0.7) +
  gghighlight(is.na(cum), label_key = type, use_direct_label = FALSE) +
  scale_y_log10() +
  scale_x_log10() +
  theme_simple(8) +
  xlab("length (loc or lines)") +
  ylab("cummulative % methods") +
  theme(plot.margin = unit(c(5,0,0,0), "mm"))
```






```{r}
cum_dis_data <- loc_freq_means %>%
  filter(type == "geo.mean") %>%
  select(c(length, cum_p))

loc_from_u <- function (u) {
  Position(function (e) { u < e }, cum_dis_data$cum_p)
}
 
random_data <- runif(100000)
loc_data <- sapply(random_data, loc_from_u)
ggplot(data.frame(num = loc_data),
       aes(x = num)) +
  geom_histogram(binwidth = 1)
sum(loc_data)

```







```{r}
my_fitted_fn <- function(loc) {
  # log(log(log(x + 1) + 1)) * 1 + 0
  
  a <- 0.515401
  b <- 0.801063
  c <- 0.930317
  d <- 4.69792
  e <- -1.38779
  f <- 0.947122
  x <- 0.515424
  y <- 0.863232
  
  a * log(b * log(c * log(loc + d) + e)+ f) * x + y
}
my_fitted_fn2 <- function(x) {
  # (1.93926 / (0.505825 + exp(-0.201356 *(x + 14.1355)))) * 1.93926 + -6.44029
  x^(1/40)
}
my_fitted_fn3 <- function(x) {
  (1.26874 / (0.760971 + exp(-0.194293 *(x + 33.747)))) * 253.043 + -420.897
}

my_fitted_fn4 <- function(x) {
  a <- 1.27777
  b <- 0.755701
  c <- -0.234819
  d <- 26.8714
  e <- 254.841
  f <- -429.91
  (a / (b + exp(c *(x + d)))) * e + f
}

my_fitted_fn5 <- function(x) {
  a <- 1.27777
  b <- 0.755701
  c <- -0.234819
  d <- 26.8714
  e <- 254.841
  f <- -429.91
  (a / (b + exp(c *(x + d)))) * e + f
}

loc_cumulative <- loc_freq

fitted <- my_fitted_fn(loc_freq$length)
fitted2 <- my_fitted_fn2(loc_freq$length)
fitted3 <- my_fitted_fn3(loc_freq$length)
fitted4 <- my_fitted_fn4(loc_freq$length)
fitted5 <- my_fitted_fn5(loc_freq$length)
loc_cumulative$fitted <- fitted
loc_cumulative$fitted2 <- fitted2
loc_cumulative$fitted3 <- fitted3
loc_cumulative$fitted4 <- fitted4
loc_cumulative$fitted5 <- fitted5

df <- loc_cumulative %>%
    filter(length > 1)
ggplot(df, aes(x = length)) +
  geom_point(aes(y = cum_p), size = 2, color="blue", alpha = 0.4) +
  # geom_point(aes(y = fitted),  size = 1.1, color="orange4") +
  # geom_point(aes(y = fitted2), size = 1.3, color="orange") +
  # geom_point(aes(y = fitted3), size = 1.2, color="red") +
  geom_point(aes(y = fitted4), size = 1.2, color="green") +
  geom_point(aes(y = fitted5), size = 1.1, color="red") +
  # scale_y_log10() + scale_x_log10() +
  ylab("cummulative number of methods") + xlab("lines of code") + theme_simple(8) + theme(plot.margin = unit(c(5,0,0,0), "mm"))


```

```{r}
library(minpack.lm)

start <- list(a = 1.26874, b = 0.760971, c=-0.194293, d=33.747, e = 253.043, f=-420.897)
# start <- list(a = 1, b = 1, c=-0.2, d=20, e=1, f=0)
model <- nlsLM(cum_p ~ (a / (b + exp(c *(length + d)))) * e + f, data=df, start=start, trace=TRUE,
             control=nls.control(
               # minFactor = 1/(1024 * 1024 * 1024),
               maxiter = 1000))


start <- list(a = 1, b = 1, c=1, d=1, e=1, f=0, x=1, y=0)
model <- nlsLM(cum_freq_p ~ (a * log(b * log(c * log(loc + d) + e)+ f)) * x + y, data=df, start=start, trace=TRUE,
             control=nls.control(
               # minFactor = 1/(1024 * 1024 * 1024),
               maxiter = 1000))

```

```{r}
a <- 1.26874
b <- 0.760971
c <- -0.194293
d <- 33.747
e <- 253.043
f <- -420.897
inverse_fitted <- function(u) {
  (log(-(a*e / (f - u)) - b) + c * d) / c
}

random_data <- runif(length(methods_data$linesOfCode))
loc_data <- sapply(random_data, inverse_fitted)
ggplot(data.frame(num = loc_data),
       aes(x = num)) +
  geom_histogram()
sum(loc_data)

```





```{r}
### Distribution of Method Length
# fit <- PPS.fit((methods_short %>% filter(linesOfCode > 0))$linesOfCode)
# plot(fit)

pfit <- pareto.fit((methods_data %>% filter(linesOfCode > 0))$linesOfCode)
#  #sigma is scale, scale is b
plot(pfit)

### Attempting to generate Pareto distributed random numbers

# gen_nums <- runif(length(methods_short$linesOfCode))
# a <- pfit$estimate$lambda
# b <- pfit$estimate$sigma
# 
# inv_fun_denom <- (1 - gen_nums) ^ (1/a)
# pareto_nums <- (b / inv_fun_denom) - b
# 
# pareto_data <- data.frame(linesOfCode = pareto_nums)
# 
# ggplot(pareto_data, aes(x=linesOfCode)) +
#   geom_histogram()
# 
# pfit$estimate$lambda
# pfit$estimate$sigma

```




```{r}

bins <- seq(0, max(loc))
# bins <- 0:31
loc <- methods_data$linesOfCode
bins_data <- sapply(bins, function(i) {
  length(loc[loc == i])
})

df <- data.frame(bins = bins, num = bins_data) %>%
  filter(num > 0)
ggplot(df, aes(x = bins, y = num)) +
  geom_point() +
  scale_y_log10() +
  scale_x_log10() +
  theme_simple(8) +
  ylab("number of methods") +
  xlab("lines of code") +
  theme(plot.margin = unit(c(5,0,0,0), "mm"))


my_fitted_fn <- function(x) {
  -0.00003818 * exp(3.63181043 * x)
}

```
```{r}
my_fitted_fn <- function(x) {
  #1000000 * exp(x / -5) / x^3
  #500000 / x^2 * exp(-x/27)
  # 800000 / x^2.5 * exp(-x/30)
  #773905.529245 / x^2.7 * exp(-x/100)
  #94338.9316 / x^0.6343 * exp(-x/7.5584)
  92695.9275831 / x^0.6132192 * exp(x / -7.3835094)
}
fitted <- my_fitted_fn(bins)
df <- data.frame(bins = bins, num = bins_data, fitted = fitted) %>%
    filter(num > 10 & bins > 1 & num > 30 & bins <= 30)
ggplot(df, aes(x = bins)) +
  geom_point(aes(y = num), color="blue") +
  geom_point(aes(y = fitted), color="orange") +
  scale_y_log10() + scale_x_log10() + 
  ylab("number of methods") + xlab("lines of code") + theme_simple(8) + theme(plot.margin = unit(c(5,0,0,0), "mm"))

# View(fitted)
```

```{r}
start <- list(a = 800000, b = 2.5, c=-30)
model <- nls(num ~ a / bins^b * exp(bins / c), data=df, start=start, trace=TRUE,
             control=nls.control(
               minFactor = 1/(1024 * 1024 * 1024),
               maxiter = 1000))
library(formula.tools)
invert(num ~ a / bins^b * exp(bins / c))
```


```{r}
n <- nrow(methods_data)
numbers <- runif(n)

df <- data.frame(num = numbers)
ggplot(df, aes(x=num)) + geom_histogram() + geom_density()

# double legacy_standard_exponential(aug_bitgen_t *aug_state) {
#   /* We use -log(1-U) since U is [0, 1) */
#   return -log(1.0 - legacy_double(aug_state));
# }
# 
# double legacy_pareto(aug_bitgen_t *aug_state, double a) {
#   return exp(legacy_standard_exponential(aug_state) / a) - 1;
# }

std_exp <- function(U) {
  #-log(1.0 - U)
  -log1p(-U)
}

std_exp_num <- std_exp(numbers)
df <- data.frame(num = std_exp_num)
ggplot(df, aes(x=num)) + geom_histogram() + geom_density()

pareto <- function (U, a) {
  # exp(std_exp(U) / a) - 1
  expm1(std_exp(U) / a)
}

pareto_num <- expm1(pareto(numbers, 50)) * 50000
df <- data.frame(num = pareto_num)
grid.arrange(
  ggplot(df, aes(x=num)) + geom_histogram(binwidth = 10) + geom_density(),
  ggplot(methods_data, aes(x=linesOfCode)) + geom_histogram(binwidth = 10) + geom_density(),
  ncol = 2
)
```

## TODO:
- manually determine the numbers of methods with 0-30 LOC

```{r}
bins <- 0:30
# bins <- 0:31
loc <- methods_data$linesOfCode
bins_data <- sapply(bins, function(i) {
  if (i < 31) {
    length(loc[loc == i])
  } else {
    sum(loc[loc >= 30]) / 30
  }
})

ggplot(data.frame(bins = bins, num = bins_data), aes(x = bins, y = num)) +
  # geom_col()
  geom_point() +
  scale_y_log10() +
  scale_x_log10()
  


total <- sum(bins_data)
parts <- bins_data / total
ggplot(data.frame(bins = factor(bins), num = parts), aes(x = bins, y = num)) +
  # geom_col()
  geom_point()


prefix_sum_parts <- cumsum(parts)
ggplot(data.frame(bins = factor(bins), num = cumsum(parts)), aes(x = bins, y = num)) + geom_col()
sum(parts)


loc_from_u <- function (u) {
  Position(function (e) { u < e }, prefix_sum_parts)
}
 
random_data <- runif(length(methods_data$linesOfCode))
loc_data <- sapply(random_data, loc_from_u)
ggplot(data.frame(num = loc_data),
       aes(x = num)) +
  geom_histogram(binwidth = 1)
sum(loc_data)

```
squeeze all methods into 0-30LOC methods
see what distribution looks like.
determine the ratios for each of the 0-30 segements.
Uses that to split the uniform range and distribute random numbers
