---
#!./min-iterations.knit.sh
title: "Is This Noise, or Does This Mean Something? #benchmarking"
output:
  html_notebook: default
  # html_fragment:
  #   theme: null
  #   self_contained: no
  # md_document:
  #   variant: gfm+raw_html+backtick_code_blocks  # not compatible with gfm: +markdown_in_html_blocks
  #   preserve_yaml: yes
date: '2020-07-05 21:41:22 +0100'
layout: post
categories:
  - Research
tags:
  - Benchmarking
  - just-in-time compilation
  - KnitR
  - Language Implementation
  - Research
  - Performance
  - SOM
  - SOMns
custom_scss: |
  figure { margin-left: 0; margin-right: 0; }
  img { max-width: 100%; }
  .all-data-plots .suite-name, .all-data-plots .exe-name {
    display: none;
  }
  .all-data-plots {
    position: absolute;
    left: 56em;
  }
  .all-data-plots .benchmarks {
    max-height: 50em;
    overflow: scroll;
  }
  
  .all-data-plots img {
    max-width: 15em;
  }
  
  .all-data-plots .benchmark {
    padding-bottom: 1em;
  }
  
  .post-content table th, .post-content table tr {
    font-size: 12px;
  }
  
  .post-content td:nth-child(1) {
    line-break: anywhere;
    min-width: 8em;
  }

htmlmeta:
  description: An analysis of the quality of benchmarking data.
  keywords: Benchmarking, just-in-time compilation, KnitR, Language Implementation, Research, Performance, SOM, SOMns
  author: Stefan Marr
---

## Do my performance measurements allow me to conclude anything at all?

For the work on various research languages ([SOMns], [SOM], [TruffleSOM] ...), I have
been using continuous performance tracking for the better part of the last decade.

One would hope that I would know what I am doing...

Since about half a year, I am using [ReBenchDB] to track performance (before I used [Codespeed](https://github.com/tobami/codespeed)),
and use the same statistical tools, that I am using for academic research
papers. While ReBenchDB is of great help, and built for exactly what I want to do,
it made me question more strongly than before whether I can even trust these
benchmark results.

<figure>
<img src="min-iterations-figures/rebenchdb.png" height="300">
<figcaption>Screenshort of ReBenchDB showing the results of an experiment</figcaption>
</figure>

The screenshot shows a table with the results of an experiment
for which I would like to know whether it changes the performance
of my language implementation.
I picked a section [from the full report](https://rebench.stefan-marr.de/compare/SOMns/546c1f8e011e1c4671f375b7dc76c97acfb4e428/ef8d51508b48b00f79bc50a63712b174745d103d) showing microbenchmarks, since I would expect
them to be the simplest to explain, and for this particular experiment,
I wouldn't expect any changes.

On first inspection, the results seem however mixed.
We got two benchmarks marked as red and three as green,
which may indicate slowdowns and speedups.
Though, a closer look at the boxplots indicates that we can't really draw
any strong conclusions. The boxes pretty much overlap, and my experience tells 
me that this may as well just be noise. I have seen similar ups and downs
rather frequently on these benchmarks.

This means, I can't tell with confidence whether the change
I tried to benchmark has any kind of positive or negative impact on
performance. Though, I am somewhat sure that nothing went terribly wrong.
I guess that's at least something.

One option may perhaps be to let the benchmarks run for longer.
Currently, it takes about 40min for the benchmark setup to run.
If the reliability of the results could be increased,
perhaps running the benchmarks for an hour may still be good enough.
Much longer run times will just mean that benchmarking will be ignored,
so, there's really an upper limit on how long things can take.
Another option might be adding more benchmarking machines, though,
currently, there's no budget for that either.

So, what would it take to improve the reliability of my benchmarks
and get numbers that I can trust more readily?

### Gathering More Data to Understand What's Going On

The benchmarking setup I am using relies on [ReBench]. This means,
in the ideal case, I can simply ask ReBench to execute every benchmark for 3000
iterations and repeat this 30. Since I know that compilation triggers after
1000 iterations, this should give us fully warmed up execution.
Having everything executed 30 times, i.e., using 30 invocations should make sure that
any variation across different invocations can also be determined reliably.

This should be all we need to change about the setup:

```shell
rebench --invocation=30 --iteration=3000 rebench.conf
```

About 7.5 days later, I got the data I wanted.

Not something that's practical for everyday development...

### Get a Bit of an Overview of All the Data

Having the data, let's try to get an impression of the overall situation.
A line chart that shows the iterations on the x-axis and the run time of
an iteration in milliseconds on the y-axis should do the job.

Though, we got 30 invocations for every benchmark. To see how these invocations
differ, I'll include all but make the lines transparent. This should allow
us to see differences in behavior.

```{r setup, echo=FALSE, include=FALSE}
if (!suppressPackageStartupMessages(library(here, logical.return=TRUE))) {
  install.packages("here", repos="https://cloud.r-project.org/")
  library(here)
}
source(here("libs", "common.R"), chdir = TRUE)


knitr::opts_chunk$set(
  echo = FALSE,
  dev = "ragg_png",
  dpi = 192) #  dpi = 192

knitr::knit_hooks$set(
  plot = function(x, options) {
    cap  <- options$fig.cap  # figure caption
    tags <- htmltools::tags
    as.character(tags$img(src = x, alt = cap))
  }
)

# data1 <- load_data("https://rebench.stefan-marr.de/rebenchdb/get-exp-data/137")
data1 <- load_data("https://rebench.stefan-marr.de/rebenchdb/get-exp-data/37")
#data2 <- load_data("https://rebench.stefan-marr.de/rebenchdb/get-exp-data/36")


warmup_plot <- function (data_b, upper_bound = 3, highlight_invocation = 4) {
  ## First take the medians over the values for each commitid separately
  ## use the highest one with a little margin as an upper bound
  upperBound <- (data_b %>%
    dplyr::group_by(commitid) %>%
    dplyr::summarise(median = median(value), .groups = "drop_last") %>%
    dplyr::summarise(upperBound = max(median) * upper_bound))$upperBound
  
  plot <- ggplot(data_b, aes(x=iteration, y=value)) +
    coord_cartesian(ylim=c(0, upperBound)) +
    geom_vline(
      xintercept = seq(0, max(data_b$iteration), 100),
      linetype = "longdash", colour = "#dddddd") +

    geom_line(aes(group = invocation), color = "#0000ff", alpha = 0.05) +
    ylab(levels(data_b$unit)) +
    xlab("iterations") +
    theme_simple(8) +
    theme_no_legend() +
      theme(axis.title.x = element_text(margin = margin(t = -6))) +
    scale_force_origin_continuous()
  if (is.numeric(highlight_invocation)) {
    plot <- plot + geom_line(
      data = data_b %>% filter(invocation == 4),
      size=0.1,
      color = "#cc0000", alpha = 1)
  }
  
  plot <- plot +
    # after this iteration, we consider data hot
    geom_vline(
      xintercept = 2000,
      linetype = "longdash", colour = "#cc0000")
  
  plot
}

steady_suites <- c("macro-steady", "micro-steady", "savina-jit")

data_f <- data1 %>%
  ## exclude interpreter results, and also SOMns-native. It's just more stuff
  ##  but not really contributing to the goal
  dplyr::filter(suite %in% steady_suites, exe != "SOMns-native") %>%
  dplyr::select(-c(cores, extraargs, criterion, warmup, varvalue, cmdline, inputsize,
                   trialid, runid, expid)) %>%
  droplevels()
data_f$invocation <- factor(data_f$invocation)
```

The first plot below shows the [IntegerLoop](https://github.com/smarr/SOMns/blob/release/core-lib/Benchmarks/LanguageFeatures.ns#L192-L197) 
benchmark:

```Smalltalk
public benchmark = ( | bounds a |
  bounds:: 20000.
  bounds negated to: bounds by: 1 do: [:value | a:: value - value].
  ^ a
)
```

The above code is a loop from -20000 to 20000 that sets the variable `a`
to the result of subtracting the current loop index from itself.
It's a classic microbenchmark
and I wouldn't expect too many complications from it.

```{r integer-loop-all-data, results='asis', cache=TRUE, fig.height=5, fig.width=8}
cp("<figure>")
warmup_plot(
  data_f %>% filter(bench == "IntegerLoop", suite == "micro-steady", exe == "SOMns-graal"),
  highlight_invocation = NA)
cp("<figcaption>Run time of iterations for IntegerLoop benchmark on SOMns</figcaption>")
cp("</figure>")
```

Though, the plot shows a few interesting bits. First note that there is a
dark blue line coming down and then settling at about 25ms.
At iteration 1000, we can see a clear spike. At this point, the
benchmark harness itself crossed the compilation threshold and got compiled.
Before, only our `benchmark` method from above was compiled,
since it was executed in an additional loop in the harness.
Unfortunately, by compiling the harness as well, performance seems to
decrease a bit to about 30ms, i.e. a 20% increase in run time.
Since compilers rely on heuristics,
these things can happen.
Though, it would be good to look into why it happens here.

But back to the plot itself. We also see light blue spikes,
in different shades of blue, and different sizes. These spikes likely
correspond to garbage collection or compilation events.
Though, overall, the result is pretty stable. Except for the undesirable slowdown,
we would hope that all benchmarks look like this, since all thirty invocations would
report the same result.

The next plot below shows the
[Vacation](https://github.com/smarr/SOMns/blob/release/core-lib/Benchmarks/Vacation.ns) benchmark,
which is a simulation of a travel booking system that has been used for instance to
benchmark software transactional memory systems.
The version used here, relies on threads and locks, even though, we run it only
with a single thread.

```{r vacation-data, results='asis', cache=TRUE, fig.height=5, fig.width=8}
cp("<figure>")
warmup_plot(
  data_f %>% filter(bench == "Vacation", suite == "macro-steady", exe == "SOMns-graal"))
cp("<figcaption>Run time of iterations for Vacation benchmark on SOMns</figcaption>")
cp("</figure>")
```

```{r all-data, results='asis', fig.height=2.5, fig.width=3.5, echo=FALSE, cache=TRUE, dpi=120}
cp("<div class=\"all-data-plots\">")
cp("<p><strong>All Iteration Plots</strong></p><div class=\"benchmarks\">")
for (e in levels(data_f$exe)) {
  data_e <- data_f %>%
    dplyr::filter(exe == e) %>%
    droplevels()
  
  # for (s in levels(data_e$suite)) {
  for (s in steady_suites) {
    data_s <- data_e %>%
      dplyr::filter(suite == s) %>%
      droplevels()
    
    for (b in levels(data_s$bench)) {
      data_b <- data_s %>%
        dplyr::filter(bench == b) %>%
        droplevels()
      
      cp('<div class="benchmark"><span class="benchmark-name">', b, '</span><span class="suite-name">', s, '</span><span class="exe-name">', e, '</span>')
      cp('<div class="plot">')
      print(warmup_plot(data_b, highlight_invocation = 3))
      cp('</div></div>')
    }
  }
}
cp("</div></div>")
```

The plot is clearly less nice than the one for IntegerLoop.
Only after about a 100 iterations, we reach a point where the measurements fit onto the plot.
This shows that the compiler can optimize the benchmark very well, but it takes time to do so.
Eventually, the benchmark reaches a run time of under 2ms, which isn't ideal,
since we might simply be measuring noise at this level of performance.

We also see that there is a much larger area that is different shades of blue.
Though, there seems to be a blue core, which indicates that there is a
somewhat normal-ish distribution of iteration times across our 30 invocations
of the benchmark.

Just to emphasize what is visualized, I picked out one invocation,
and instead of plotting it in a transparent blue, it's plotted in red.
With it we can see the ups and downs within a single
invocation, but the overall trend seems to stabilize.

The plots for all benchmarks are shown in the box on the right.

### How Close Can We Get to the "True" Average Performance?

After inspecting our plots, and considering the compilation threshold of 1000
iterations, it seems plausible to define the "true" average for our benchmarks
based on the measurements after the 2000th iteration. 
In the previous plots, you may have noticed the red dashed line at the 2000th
iteration. This definition gives us 1000 iterations to calculate the average
from. As a somewhat stable average, I'll use the median for now.
Let's assume for simplicity that this is fine.


```{r calc-hot-stats, echo=FALSE, cache=TRUE}
hot <- data_f %>%
  dplyr::filter(iteration > 2000) %>%
  droplevels()

calc_all_stats <- function(value) {
  median <- median(value)
  mean   <- mean(value)
  min    <- min(value)
  q      <- quantile(value, probs = c(0.05, 0.10, 0.15, 0.20, 0.25))

  data.frame(median = median,
             mean = mean,
             min = min,
             q5 = q[[1]], q10 = q[[2]], q15 = q[[3]], q20 = q[[4]], q25 = q[[5]]
             )
}

name_total_hot <- function(df) {
  colnames(df) <- paste0("total_hot_", colnames(df))
  df
}

stats_hot <- hot %>%
  dplyr::group_by(exe, suite, bench) %>%
  dplyr::summarise(
    name_total_hot(calc_all_stats(value)),
    .groups = "drop"
  )

```

Since the time benchmarking takes needs to be practical,
I need to to minimize this warmup threshold as well as the number of iterations
used to calculate the median, while keeping it within an acceptable range of
the "true" median I calculated from the last 1000 iterations.

To get a feeling of whether this is workable, I'll visualize how far off
the median is when I take various different warmup thresholds and
number of iterations to calculate the average.

```{r calc-warmup-stats, echo = FALSE, cache=TRUE}
warmup <- seq(1, 2001, by = 25)
iter   <- seq(1, 1001, by = 25)
params <- crossing(warmup, iter)
param_list <- split(params, seq(nrow(params)))

plan(multisession, workers = availableCores())

# Sequential version:  stats_for_params <- purrr::map(param_list, function (p) {
stats_for_params <- future_map(param_list, function (p, stats_hot, data_f) {
  stats <- data_f %>%
    dplyr::filter(iteration >= p$warmup & iteration <= (p$warmup + p$iter)) %>%
    droplevels() %>%
    dplyr::group_by(exe, suite, bench) %>%
    dplyr::summarise(median_ = median(value), .groups = "drop") %>%
    left_join(stats_hot, by = c("exe", "suite", "bench")) %>%
    dplyr::mutate(diff_to_hot_median = abs(total_hot_median - median_),
                  diff_to_hot_median_ratio = abs(total_hot_median - median_) / total_hot_median)
    
  list(max_iter = p$warmup + p$iter,
       warmup   = p$warmup,
       stats    = stats)
}, stats_hot = stats_hot, data_f = data_f)

## end the parallel workers. mostly useful for interactive sessions
# future:::ClusterRegistry("stop")
```
    
```{r fn-plot-warmup-params, echo=FALSE, cache=TRUE}
color_palette <- colorRampPalette(colors = c("blue", "yellow", "red"))(length(warmup))

extract_stats_for_b <- function (d, b, s, e) {
  max_iter <- d$max_iter
  iter     <- d$max_iter - d$warmup
  warmup   <- d$warmup
  
  data_for_b <- d$stats %>%
    filter(bench == b, suite == s, exe == e) %>%
    droplevels()
  data.frame(max_iter, iter, warmup, data_for_b)
}

filter_diagonal <- function(warmup, iter) {
  # the -1 are because the values start counting at 1 which makes this a little awkward...
  # using the abs <= 25 gives use a wide step function with more data,
  # but without any duplicates for max_iter, so, it should work for plotting
  abs((warmup - 1) - (2 * (iter - 1))) <= 25
}

plot_median_diff_from_hot <- function(b, s, e, legend_height = 20) {
  for_viz <- purrr::map_dfr(stats_for_params, extract_stats_for_b, b, s, e)

  for_viz_filtered <- for_viz %>%
    filter(diff_to_hot_median_ratio <= 0.05) %>%
    mutate(diff = diff_to_hot_median_ratio * 100,
           marked = filter_diagonal(warmup, iter))

  plot <- ggplot(for_viz_filtered, aes(x=iter, y=warmup)) +
      geom_tile(aes(fill = diff)) +
      scale_fill_gradientn(colors = color_palette, limits = c(0, 5), name = "% diff") + #n.breaks = 10,
      guides(fill = guide_colourbar(barheight = legend_height, nbins=10)) +
      xlab("num. iterations") +
      geom_point(shape = 3, alpha = 0.4,
                 data = for_viz_filtered %>% filter(marked)) +
      # scale_shape_manual(values = c("marked"=3, "unmarked"=FALSE)) +
      theme_simple(8) +
        theme(legend.title = element_text(size = 8))
  print(plot)
}
```

<!--
#### How to Read these Plots?

- blue indicates the closest match with the determine true median
  (based on the `hot` stats)
- red is indicating a larger difference
- note that the scale adapts to the benchmark
- all data above 5% from true median is filtered out for better visualization
-->

The plot below depicts the difference to the "true" median in percent.
A clear blue means that the median calculated from after discarding the
warmup iteration on the y-axis and using only the first n iterations (x-axis)
is identical to the "true" median. A clear bright red means the median differs
by 5%. If no color is given, the median differs by more than 5%.

```{r int-loop-median-diff-from-hot, results='asis', echo=FALSE, fig.height=5, fig.width=7, cache=TRUE}
cp("<figure>")
plot_median_diff_from_hot("IntegerLoop", "micro-steady", "SOMns-graal")
cp("<figcaption>Difference from \"true\" median in % for IntegerLoop</figcaption>")
cp("</figure>")
```

This means, for our IntegerLoop benchmark, we can match the figure easily to
our previous figure. Because of how the compilation decided to optimize,
we have initially a performance that differs by about 20% from the "true" median
and thus, is left white.
Around the 1000th iteration, discarding them as warmup,
and using only perhaps 100 iterations for the mean, we see some yellow, which
means the mean is off by 2.5%. Below 1000 iterations of warmup, but with more
than a 100 or so iterations, we see a darker shade of blue, indicating perhaps
1% difference from the mean. The rest is clear blue, indicating that which ever
configuration of warmup and iterations we chose would match the true median.

```{r vacation-median-diff-from-hot, results='asis', echo=FALSE, fig.height=3, fig.width=7, cache=TRUE}
cp("<figure>")
plot_median_diff_from_hot("Vacation", "macro-steady", "SOMns-graal")
cp("<figcaption>Difference from \"true\" median in % for Vacation</figcaption>")
cp("</figure>")
```

```{r median-diff-from-hot-all-data, results='asis', echo=FALSE, fig.height=2.5, fig.width=3.5, cache=TRUE}
cp("<div class=\"all-data-plots\">")
cp("<p><strong>All Warmup/Iteration Plots</strong></p><div class=\"benchmarks\">")

#options(warn=0)
#warning=FALSE, error=FALSE, 
# b <- "CD", s <- "macro-steady", e <- "SOMns-graal"

for (e in levels(data_f$exe)) {
  data_e <- data_f %>%
    dplyr::filter(exe == e) %>%
    droplevels()
  
  # for (s in levels(data_e$suite)) {
  for (s in steady_suites) {
    data_s <- data_e %>%
      dplyr::filter(suite == s) %>%
      droplevels()
    
    for (b in levels(data_s$bench)) {
      cp('<div class="benchmark"><span class="benchmark-name">', b, '</span><span class="suite-name">', s, '</span><span class="exe-name">', e, '</span>')
      cp('<div class="plot">')
      
      tryCatch(
        plot_median_diff_from_hot(b, s, e, legend_height = 10),
        error = function(ex) {
          cp('Error for ', b, ' ', s, ' ', e)
          print(ex)
          cp(ex)
        }
      )
      
      cp('</div></div>')
    }
  }
}
cp("</div></div>")
```

For the Vacation benchmark, the plot is more interesting.
For the most parts, we are far off the "true" median. Though, then with
increasing number of warmup and used iterations, we get close to it.
This establishes a diagonal where the values become gradually more accurate.

Looking at these plots for all benchmarks in the box on the right,
we see that for quite a number of them,
things look pretty good, and they have a lot of blue. Though, there are also
a number, where we clearly see that the compilation threshold is an issue,
and we only get closer to the "true" median, after we crossed the threshold.
For other benchmarks, there are a lot of parts that do not come close to the
"true" median, like for the Vacation benchmark.

### Is There A More Stable and Informative Metric?

While the above plots give us some insight in how much warmup and number of
iterations are needed to gets close to the "true" median,
the precision leaves a bit to be desired.
To make these plots readable, I also avoided to add other colors to distinguish
between values below and above the "true" median.

Originally, I wanted to use [bootstrapping](https://en.wikipedia.org/wiki/Bootstrapping_(statistics))
and bootstrap confidence intervals here,
which would avoid making unsupported assumptions about the distribution of
measurements.
Unfortunately, it turned out to be impractical with the large data set at hand.
Instead, I'll look at a few quantiles, the arithmetic mean, and minimum.
The minimum has been shown before as a metric that stabilizes fast, though,
I assume that it does hide possibly important changes in practice.

To investigate these metrics,
I will look at the pairs of warmup and number of iterations that fall roughly
on the diagonal of the plots in the last section, indicated by the crosses
you may have noticed.



```{r calc-stats, echo = FALSE, cache=TRUE}
# plan(multisession, workers = availableCores() / 2)

diagonal <- params %>%
  filter(filter_diagonal(warmup, iter)) %>%
  mutate(max_iter = warmup + iter)

diagonal_list <- split(diagonal, seq(nrow(diagonal)))

# based on inspection, 2000 seems a overall good value
stats_for_diagonal <- future_map(diagonal_list, function (p, stats_hot, data_f) {
  stats <- data_f %>%
    dplyr::filter(iteration >= p$warmup & iteration <= (p$warmup + p$iter)) %>%
    droplevels() %>%
    dplyr::group_by(exe, suite, bench) %>%
    dplyr::summarise(calc_all_stats(value),  .groups = "drop") %>%
    left_join(stats_hot, by = c("exe", "suite", "bench"))

  list(max_iter = p$warmup + p$iter,
       warmup   = p$warmup,
       stats    = stats)
}, stats_hot = stats_hot, data_f = data_f)

# future:::ClusterRegistry("stop")

plot_stats_diff_from_hot <- function(stats_list, b, s, e) {
  for_viz <- purrr::map_dfr(stats_list, extract_stats_for_b, b, s, e) %>%
    filter(filter_diagonal(warmup, iter)) %>%
    dplyr::mutate(
      diff_median = ((total_hot_median - median) / total_hot_median) * 100,
      diff_mean   = ((total_hot_mean   - mean)   / total_hot_mean)   * 100,
      diff_min    = ((total_hot_min    - min)    / total_hot_min)    * 100,
      diff_q5   =   ((total_hot_q5     - q5)     / total_hot_q5)     * 100,
      diff_q10  =   ((total_hot_q10    - q10)    / total_hot_q10)    * 100,
      diff_q15  =   ((total_hot_q15    - q15)    / total_hot_q15)    * 100,
      diff_q20  =   ((total_hot_q20    - q20)    / total_hot_q20)    * 100,
      diff_q25  =   ((total_hot_q25    - q25)    / total_hot_q25)    * 100
    )
  
  plot <- ggplot(for_viz, aes(x=max_iter)) +
    geom_vline(aes(xintercept =  1000), color = "#cccccc", linetype = "dashed") +
    geom_vline(aes(xintercept =  2000), color = "#cccccc", linetype = "dashed") +
    geom_hline(aes(yintercept =    1), color = "#999999", linetype = "dashed") +
    geom_hline(aes(yintercept =   -1), color = "#999999", linetype = "dashed") +
    geom_hline(aes(yintercept =  0.1), color = "#999999", linetype = "dashed") +
    geom_hline(aes(yintercept = -0.1), color = "#999999", linetype = "dashed") +
    geom_line(aes(y = diff_min,    color = "min"),    alpha = 0.5) +
    geom_line(aes(y = diff_q5,     color = "q05"),    alpha = 0.5) +
    geom_line(aes(y = diff_q10,    color = "q10"),    alpha = 0.5) +
    geom_line(aes(y = diff_q15,    color = "q15"),    alpha = 0.5) +
    geom_line(aes(y = diff_q20,    color = "q20"),    alpha = 0.5) +
    geom_line(aes(y = diff_q25,    color = "q25"),    alpha = 0.5) +
    geom_line(aes(y = diff_mean,   color = "mean"),   alpha = 0.5) +
    geom_line(aes(y = diff_median, color = "median"), alpha = 0.5) +
    scale_color_manual(values = c(
      median = "#66c2a5",
      mean = "#fc8d62",
      min = "#dadaeb",
      q05 = "#bcbddc",
      q10 = "#9e9ac8",
      q15 = "#807dba",
      q20 = "#6a51a3",
      q25 = "#4a1486"
    )) +
    coord_cartesian(ylim = c(-5, 5)) +
    theme_simple(8) +
    ylab("Difference from \"true\" value in %") +
    xlab("warmup + num. iterations")
  print(plot)
}

```

As before, the first plot shows the results for IntegerLoop.
The plot focuses on the range of 5% above and 5% below the "true" value assumed
for each metric.

```{r int-loop-diff-from-hot, results='asis', echo=FALSE, fig.height=5, fig.width=7, cache=TRUE}
cp("<figure>")
plot_stats_diff_from_hot(stats_for_diagonal, "IntegerLoop", "micro-steady", "SOMns-graal")
cp("<figcaption>Difference from \"true\" value in % for IntegerLoop using data from all 30 invocations</figcaption>")
cp("</figure>")
```

Because of the difference of the later performance to the first 1000 iterations,
we see the metrics converging to the "true" value only after the 1000th iteration.
Interestingly, the median start converging first, while the minimum comes in later.
This happens here because we first need to reach the point where we discard
enough warmup iterations, before the minimum is what we expect it to be
based on the last 1000 iterations.

```{r stats-diff-from-hot, results='asis', fig.height=3.0, fig.width=6.0, echo=FALSE, cache=TRUE}
#options(warn=0)
#warning=FALSE, error=FALSE, 
# b <- "CD", s <- "macro-steady", e <- "SOMns-graal"
cp("<div class=\"all-data-plots\">")
cp("<p><strong>All Difference Plots</strong></p><div class=\"benchmarks\">")
for (e in levels(data_f$exe)) {
  data_e <- data_f %>%
    dplyr::filter(exe == e) %>%
    droplevels()
  
  # for (s in levels(data_e$suite)) {
  for (s in steady_suites) {
    data_s <- data_e %>%
      dplyr::filter(suite == s) %>%
      droplevels()
    
    for (b in levels(data_s$bench)) {
      cp('<div class="benchmark"><span class="benchmark-name">', b, '</span><span class="suite-name">', s, '</span><span class="exe-name">', e, '</span>')
      cp('<div class="plot">')
      
      plot_stats_diff_from_hot(stats_for_diagonal, b, s, e)
      cp('</div></div>')
    }
  }
}
cp("</div></div>")
```

```{r vacation-diff-from-hot, results='asis', echo=FALSE, fig.height=5, fig.width=7, cache=TRUE}
cp("<figure>")
plot_stats_diff_from_hot(stats_for_diagonal, "Vacation", "macro-steady", "SOMns-graal")
cp("<figcaption>Difference from \"true\" value in % for Vacation using data from all 30 invocations</figcaption>")
cp("</figure>")
```

For the Vacation benchmark, the plot again looks very different.
As we would expect from previous plots, the metrics converge very late.
Though, here the minimum and the lower quantiles converge before
the median and mean.

The other plots are again in a box on the right.

### What's the Largest Differnce from the "True" Value for these Metrics?

The above plots give use an idea of what it looks like
when we use the data of 30 invocations.
Though, realistically, this takes too long for day-to-day engineering.
Below, I am going to look at how the metrics behave when we always pick
the invocation with the largest difference from
the "true" value.

```{r calc-stats-per-in, echo = FALSE, cache=TRUE}
stats_for_diagonal_i <- future_map(diagonal_list, function (p, stats_hot, data_f) {
  stats <- data_f %>%
    dplyr::filter(iteration >= p$warmup & iteration <= (p$warmup + p$iter)) %>%
    droplevels() %>%
    dplyr::group_by(exe, suite, bench, invocation) %>%
    dplyr::summarise(calc_all_stats(value),  .groups = "drop") %>%
    left_join(stats_hot, by = c("exe", "suite", "bench")) %>%
    dplyr::group_by(exe, suite, bench, invocation) %>%
    dplyr::mutate(
      diff_median = ((total_hot_median - median) / total_hot_median) * 100,
      diff_mean   = ((total_hot_mean   - mean)   / total_hot_mean)   * 100,
      diff_min    = ((total_hot_min    - min)    / total_hot_min)    * 100,
      diff_q5   =   ((total_hot_q5     - q5)     / total_hot_q5)     * 100,
      diff_q10  =   ((total_hot_q10    - q10)    / total_hot_q10)    * 100,
      diff_q15  =   ((total_hot_q15    - q15)    / total_hot_q15)    * 100,
      diff_q20  =   ((total_hot_q20    - q20)    / total_hot_q20)    * 100,
      diff_q25  =   ((total_hot_q25    - q25)    / total_hot_q25)    * 100
    ) %>%
    dplyr::group_by(exe, suite, bench) %>%
    dplyr::summarise(
      max_median = max(abs(diff_median )),
      max_mean   = max(abs(diff_mean  )),
      max_min    = max(abs(diff_min   )),
      max_q5     = max(abs(diff_q5    )),
      max_q10    = max(abs(diff_q10   )),
      max_q15    = max(abs(diff_q15   )),
      max_q20    = max(abs(diff_q20   )),
      max_q25    = max(abs(diff_q25   )),
      .groups = "drop")

  list(max_iter = p$warmup + p$iter,
       warmup   = p$warmup,
       stats    = stats)
}, stats_hot = stats_hot, data_f = data_f)

# future:::ClusterRegistry("stop")

plot_i_stats_diff_from_hot <- function(stats_list, b, s, e, y_limit = c(0, 5)) {
  for_viz <- purrr::map_dfr(stats_list, extract_stats_for_b, b, s, e)
  
  plot <- ggplot(for_viz, aes(x=max_iter)) +
    geom_vline(aes(xintercept =  1000), color = "#cccccc", linetype = "dashed") +
    geom_vline(aes(xintercept =  2000), color = "#cccccc", linetype = "dashed") +
    geom_hline(aes(yintercept =    1), color = "#999999", linetype = "dashed") +
    geom_hline(aes(yintercept =   -1), color = "#999999", linetype = "dashed") +
    geom_hline(aes(yintercept =  0.1), color = "#999999", linetype = "dashed") +
    geom_hline(aes(yintercept =    0), color = "#999999", linetype = "dotted") +
    geom_line(aes(y = max_min,    color = "min"),    alpha = 0.5) +
    geom_line(aes(y = max_q5,     color = "q05"),    alpha = 0.5) +
    geom_line(aes(y = max_q10,    color = "q10"),    alpha = 0.5) +
    geom_line(aes(y = max_q15,    color = "q15"),    alpha = 0.5) +
    geom_line(aes(y = max_q20,    color = "q20"),    alpha = 0.5) +
    geom_line(aes(y = max_q25,    color = "q25"),    alpha = 0.5) +
    geom_line(aes(y = max_mean,   color = "mean"),   alpha = 0.5) +
    geom_line(aes(y = max_median, color = "median"), alpha = 0.5) +
    scale_color_manual(values = c(
      median = "#66c2a5",
      mean = "#fc8d62",
      min = "#dadaeb",
      q05 = "#bcbddc",
      q10 = "#9e9ac8",
      q15 = "#807dba",
      q20 = "#6a51a3",
      q25 = "#4a1486"
    )) +
    coord_cartesian(ylim = y_limit) +
    theme_simple(8) +
    ylab("Difference from \"true\" value in %") +
    xlab("warmup + num. iterations")
  print(plot)
}

```

For the IntegerLoop benchmark, things look fairly familiar.
Though, the mean does seem to get worse with more data.
The median seems to be the best metric of the lot,
but stays well above the indicated 0.1% error margin.

```{r int-loop-i-diff-from-hot, results='asis', echo=FALSE, fig.height=5, fig.width=7, cache=TRUE}
cp("<figure>")
plot_i_stats_diff_from_hot(stats_for_diagonal_i, "IntegerLoop", "micro-steady", "SOMns-graal")
cp("<figcaption>Max. difference of any invocation from \"true\" median in % for IntegerLoop</figcaption>")
cp("</figure>")
```

```{r stats-i-diff-from-hot, results='asis', fig.height=3.0, fig.width=6.0, echo=FALSE, cache=TRUE}
cp("<div class=\"all-data-plots\">")
cp("<p><strong>All Max. Difference Plots</strong></p><div class=\"benchmarks\">")
for (e in levels(data_f$exe)) {
  data_e <- data_f %>%
    dplyr::filter(exe == e) %>%
    droplevels()
  
  # for (s in levels(data_e$suite)) {
  for (s in steady_suites) {
    data_s <- data_e %>%
      dplyr::filter(suite == s) %>%
      droplevels()
    
    for (b in levels(data_s$bench)) {
      cp('<div class="benchmark"><span class="benchmark-name">', b, '</span><span class="suite-name">', s, '</span><span class="exe-name">', e, '</span>')
      cp('<div class="plot">')
      plot_i_stats_diff_from_hot(stats_for_diagonal_i, b, s, e)
      cp('</div></div>')
    }
  }
}
cp("</div></div>")
```

For the Vacation benchmark, the situation is quite a bit worse.
We are getting nowhere near the accuracy that we would need.
The error does barely reach 25%.

```{r vacation-i-diff-from-hot, results='asis', echo=FALSE, fig.height=5, fig.width=7, cache=TRUE}
cp("<figure>")
plot_i_stats_diff_from_hot(stats_for_diagonal_i, "Vacation", "macro-steady", "SOMns-graal", c(0, 100))
cp("<figcaption>Difference from \"true\" value in % for Vacation using data from all 30 invocations</figcaption>")
cp("</figure>")
```


Looking at the plots for all benchmarks in the box on the right, doesn't make me feel more joyous either.
It's bad.


### How Many Benchmarks Reach a Certain Accuracy?

With these plots, the next question coming to my mind is how many benchmarks
actually reach a desired accuracy. So, for how many benchmarks is the difference
to a "true" average within an allowed range.


```{r calc-first-good-iter, echo = FALSE, cache=TRUE}
list_to_df <- function (d) {
  data_for_b <- d$stats %>%
    mutate(
      max_iter = d$max_iter,
      iter     = d$max_iter - d$warmup,
      warmup   = d$warmup
    )
  data_for_b
}

stats_for_diagonal_i_df <- purrr::map_dfr(stats_for_diagonal_i, list_to_df)

max_after <- function (values) {
  rev(cummax(rev(values)))
}

stats_after <- stats_for_diagonal_i_df %>%
  dplyr::group_by(exe, suite, bench) %>%
  mutate(max_mean_after   = max_after(max_mean),
         max_median_after = max_after(max_median),
         max_min_after    = max_after(max_min),
         max_q05_after    = max_after(max_q5),
         max_q10_after    = max_after(max_q10),
         max_q15_after    = max_after(max_q15),
         max_q20_after    = max_after(max_q20),
         max_q25_after    = max_after(max_q25))

stats_after_longer <- stats_after %>%
  pivot_longer(starts_with("max_") & ends_with("_after"),
               names_to = "max_after_stat",
               values_to = "max_after_val") %>%
  dplyr::group_by(exe, suite, bench, max_after_stat)

accuracy_params <- seq(0.1, 20, by = 0.1)
num_benchmarks_at_accuracy <- function(a, stats) {
  stats %>%
    filter(max_after_val <= a) %>%
    summarise(min_max_iter = min(max_iter), .groups = "drop") %>%
    dplyr::group_by(max_after_stat) %>%
    summarise(
      accuracy = a,
      n = n(), .groups = "drop")
}
accuracy <- purrr::map_dfr(accuracy_params, num_benchmarks_at_accuracy, stats = stats_after_longer)

plot_accuracy <- function(accuracy) {
  plot <- ggplot(accuracy, aes(x=accuracy, y=n)) +
    geom_line(aes(color = max_after_stat)) +
    scale_color_manual(values = c(
      max_median_after = "#66c2a5",
      max_mean_after = "#fc8d62",
      max_min_after = "#dadaeb",
      max_q05_after = "#bcbddc",
      max_q10_after = "#9e9ac8",
      max_q15_after = "#807dba",
      max_q20_after = "#6a51a3",
      max_q25_after = "#4a1486"
    )) +
    theme_simple(8) +
    xlab("Difference from \"true\" value in % (lower is better) ") +
    ylab("Number of benchmarks (higer is better)")
  print(plot)
}

num_experiments <- nrow(stats_after %>% dplyr::group_by(exe, suite, bench) %>% summarise(t = 1, .groups = "drop"))
```

The plot below indicates again: it doesn't look good.
The 25th percentile seems to be have reasonably well
and I'll rely on it for the further discussion.

In total, there are `r num_experiments` benchmarks.
If we are optimistic, and allow merely an error of 0.1%,
there are only `r (accuracy %>% filter(max_after_stat == "max_q25_after", accuracy == 0.1))$n` 
benchmarks that reach the goal using the 25th percentile as metric.
With a 1% error, we get up to `r (accuracy %>% filter(max_after_stat == "max_q25_after", accuracy == 1))$n` benchmarks, and with 5% even `r (accuracy %>% filter(max_after_stat == "max_q25_after", accuracy == 5))$n`.

Though this means there are a lot of benchmarks, for which the accuracy
is not very practical. Even with a 5% fluctuation, I can't really tell whether
a small optimization in the language implementation has an impact on
performance.



```{r plot-accuracy, results='asis', echo=FALSE, fig.height=5, fig.width=7, cache=TRUE}
cp("<figure>")
plot_accuracy(accuracy)
cp("<figcaption>The number of benchmars that reach a given accuracy goal,
   i.e., for a metric the benchmark's value is within the allowed difference from the \"true\" value.</figcaption>")
cp("</figure>")

```

<!--
- the 25% percentile seems to be good, median doesn't seem much worse
- for each accuracy level
  - get the max_iter per benchmark
-->

### How Much Data Do We Need to Reach a Certain Accuracy?

To conclude the discussion for now, let's look at how much data we need to reach
a desired level of accuracy.
Using the 25% percentile as before, I'll determine the number of iterations
that it takes (including warmup) before we reach a given degree of accuracy.

The table below shows the required iterations for a given accuracy.
Benchmarks that aren't listed, simply don't reach the accuracy.
For the ones that are listed, we can see that a lot of data is needed.
Of course, this is still assuming we only use a single invocation for the benchmark.
Thus, these numbers a derived from the maximum error between "true" value and
one of the observed invocations.

```{r max-iter-accuracy-table, echo=FALSE, cache=TRUE}
accuracy_params <- seq(0.1, 2.5, by = 0.1)
max_iter_at_accuracy <- function(a, stats) {
  stats %>%
    filter(max_after_val <= a) %>%
    summarise(
      accuracy = a,
      min_max_iter = min(max_iter), .groups = "drop")
}
q25_for_key_exe <- stats_after_longer %>%
  filter(max_after_stat == "max_q25_after", exe != "SOMns-native") %>%
  ungroup() %>%
  select(-c(max_after_stat, suite, exe)) %>%
  dplyr::group_by(bench)
max_iter_acc <- purrr::map_dfr(accuracy_params, max_iter_at_accuracy, stats = q25_for_key_exe)
max_iter_acc_table <- max_iter_acc %>%
  pivot_wider(names_from = accuracy, names_glue = "{accuracy}%", values_from = min_max_iter)

options(knitr.kable.NA = '')
colnames(max_iter_acc_table)[1] <- "Benchmark"
kable(max_iter_acc_table)
```




```{r prep-inv-data,             eval = FALSE, echo = FALSE}

## TODO
## - what's the diff between slowest and fastest inv stats on hot?

base <- data_f %>%
  dplyr::group_by(exe, suite, bench) %>%
  dplyr::summarise(
    base_median = median(value))


# Scaling is not useful, because it does not allow me to compare things like sd and things directly, I still need to scale those
# norm <- data_f %>%
#   left_join(base, by = c("exe", "suite", "bench")) %>%
#   dplyr::group_by(exe, suite, bench) %>%
#   dplyr::mutate(norm_val = value / base_median)

stats_i <- data_f %>%
  dplyr::group_by(exe, suite, bench, invocation) %>%
  dplyr::summarise(
    mean   = mean  (value),
    median = median(value),
    min    = min   (value),
    # max    = max   (value),
    q5     = quantile(value, probs = 0.05),
    q10    = quantile(value, probs = 0.10),
    q15    = quantile(value, probs = 0.15),
    q20    = quantile(value, probs = 0.20),
    q25    = quantile(value, probs = 0.25)
  )

stats <- data_f %>%
  dplyr::group_by(exe, suite, bench) %>%
  dplyr::summarise(
    total_mean   = mean  (value),
    total_median = median(value),
    total_min    = min   (value),
    # total_max    = max   (value),
    total_q5     = quantile(value, probs = 0.05),
    total_q10    = quantile(value, probs = 0.10),
    total_q15    = quantile(value, probs = 0.15),
    total_q20    = quantile(value, probs = 0.20),
    total_q25    = quantile(value, probs = 0.25)
  )



stats_hot_i <- hot %>%
  dplyr::group_by(exe, suite, bench, invocation) %>%
  dplyr::summarise(
    median = median(value),
    
    mean   = mean  (value),
    min    = min   (value),
    
    q5     = quantile(value, probs = 0.05),
    q10    = quantile(value, probs = 0.10),
    q15    = quantile(value, probs = 0.15),
    q20    = quantile(value, probs = 0.20),
    q25    = quantile(value, probs = 0.25)
  )

stats_hot_i_stats <- stats_hot_i %>%
  dplyr::group_by(exe, suite, bench) %>%
  dplyr::summarise(
    m_median          = median(median),
    diff_median       = max(median) - min(median),
    diff_median_ratio = (max(median) - min(median)) / median(median),
    
    diff_mean       = max(mean) - min(mean),
    diff_mean_ratio = (max(mean) - min(mean)) / median(mean),
    
    diff_min       = max(min) - min(min),
    diff_min_ratio = (max(min) - min(min)) / median(min),
    
    diff_q5        =  max(q5)  - min(q5),
    diff_q5_ratio  = (max(q5)  - min(q5)) /  median(q5),
    diff_q10       =  max(q10) - min(q10),
    diff_q10_ratio = (max(q10) - min(q10)) / median(q10),
    diff_q15       =  max(q15) - min(q15),
    diff_q15_ratio = (max(q15) - min(q15)) / median(q15),
    diff_q20       =  max(q20) - min(q20),
    diff_q20_ratio = (max(q20) - min(q20)) / median(q20),
    diff_q25       =  max(q25) - min(q25),
    diff_q25_ratio = (max(q25) - min(q25)) / median(q25)
  )

stats <- stats %>%
  left_join(stats_hot, by = c("exe", "suite", "bench"))

stats_i_stats <- stats_i %>%
  dplyr::group_by(exe, suite, bench) %>%
  dplyr::summarise(
    iDiffMean        = max(mean) - min(mean),
    iDiffMeanRatio   = (max(mean) - min(mean)) / mean(mean),
    iDiffMedian      = max(median) - min(median),
    iDiffMedianRatio = (max(median) - min(median)) / mean(median),
     
    meanSd = sd(mean),
    mean   = mean(mean),
    medianSd = sd(median),
    median = mean(median),
    minSd  = sd(min),
    min    = mean(min),
    # maxSd  = sd(max),
    # max    = mean(max),
    q5Sd   = sd(q5),
    q5     = mean(q5),
    q10Sd  = sd(q10),
    q10    = mean(q10),
    q15Sd  = sd(q15),
    q15    = mean(q15),
    q20Sd  = sd(q20),
    q20    = mean(q20),
    q25Sd  = sd(q25),
    q25    = mean(q25)
  )
```

## Further Reading

This analysis isn't done in a vacuum of course.
Proper benchmarking and evaluation are an important topic in literature,
especially when it comes to reporting results in academic papers.
[Georges at al. (2007)](https://www2.ccs.neu.edu/racket/Performance/andy-georges-paper.pdf)
have long advocated for using statistically rigorous methods for evaluating
just-in-time compiled systems. Their proposed methodology includes to take the
mean per invocation for the iterations after a certain warmup cut off.
Though, they use also confidence intervals and the coefficient of variance,
which I haven't done here.
Interestingly they report a maximum difference of 20% between the
minimum and maximum for benchmarks at peak performance.
The benchmarks I am using on SOMns seem to show higher differences.
Georges at al. also provided JavaStat, a tool to automatically determine the necessary
number of benchmark iterations to reach a desired confidence interval
width.

[Kalibera and Jones (2013)](https://kar.kent.ac.uk/33611/45/p63-kaliber.pdf)
expand on the work by investigating how to do rigorous benchmarks in reasonable time.
They advocate for the use of effect size confidence intervals as well as a visual approach
to determine that measurements are independent of each other.
They also look at the variation between invocations, which at least for one
benchmark was 30%. This lead them to propose a way to determine
how many iterations and invocations one would need to achieve a desired precision.
In an [earlier paper](https://www.cs.kent.ac.uk/pubs/2012/3233/content.pdf)
they went into the underlying technical details.

More recently, [Barrett et al.](https://dl.acm.org/doi/10.1145/3133876)
looked into one of the underlying assumptions we typically have: we hope our
systems reach a steady state. Spoiler: they often don't.
They use [Krun](https://github.com/softdevteam/krun/#readme) to control the
system on which the benchmarks are executed to minimize interference coming
from the operating system and hardware.
The work concludes with useful suggestions about sizing experiments, similar
to what [Kalibera and Jones](https://kar.kent.ac.uk/33611/45/p63-kaliber.pdf) did.
They suggest to use at the very least 10 invocations, where 15 should give reliable results.

[Bulej et al. (2020)](https://arxiv.org/abs/2001.05811) investigate how to benchmark
in noisy environments such as multitenant cloud servers.
They propose an approach that carefully synchronizes the execution of the pairs
of systems to be compared. This seems to reduce the size of their bootstrap
confidence intervals significantly.

## What's Next?

The aforementioned literature lays a foundation for statistically rigorous work.
I have been adhering to at least some of the suggestions to ensure reliable
results in my own papers. However, none of these approaches help me with my
day-to-day engineering needs. Even doing 5 or 10 invocations for each benchmark
result in overall run times of the benchmarking job that make them too slow for
good usability.

From the data it is evident that the compilation threshold is clearly observable.
The first step will be to reduce it and hope that this will lead to better results.
A very quick and dirty run with the benchmarking settings I have been using
didn't indicate any significant improvements, but a full run is still in progress.

Another knob to be turned are the benchmark parameters. Benchmarks such as
Vacation have very short iteration times, which makes them susceptible to noise.

And, then, there are the strange things, for instance with IntegerLoop,
which need to be investigated, and possibly reveal engineering issues.

Since SOMns uses Truffle and Graal, using [libgraal](https://medium.com/graalvm/libgraal-graalvm-compiler-as-a-precompiled-graalvm-native-image-26e354bee5c) may further improve things. It would hopefully avoid interference from
the compiler being compiled at run time. Similarly, perhaps there are a few
simple things to be stolen from Krun that could reduce the environmental noise.

And there are probably, a hundred other things I could try.

If you got suggestions, tips and tricks, comments, questions, or complains,
you can find me over on Twitter [@smarr](https://twitter.com/smarr).

[SOM]: https://som-st.github.io
[SOMns]: https://github.com/smarr/SOMns#readme
[ReBench]: https://github.com/smarr/ReBench#readme
[ReBenchDB]: https://github.com/smarr/ReBenchDB#readme
[TruffleSOM]: https://github.com/SOM-st/TruffleSOM#readme
