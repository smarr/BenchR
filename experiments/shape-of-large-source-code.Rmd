---
#!./shape-of-large-source-code.knit.sh
title: "The Shape of 1.7M Lines of Code"
output:
  html_notebook: default
date: '2020-12-14 23:14:29 +0000'
layout: post
categories:
  - Research
tags:
  - Smalltalk
  - Pharo
  - Language Implementation
  - Research
  - SOMns
  - Metrics
custom_scss: |
  figure { margin-left: 0; margin-right: 0; }
  figure img { margin-left: auto; margin-right: auto; display: block; }
  figure.full img { max-width: 100%; }
  figure.two-thirds img { max-width: 66%; }

htmlmeta:
  description: Analysing the shape of 1.7M of Pharo Smalltalk Code
  keywords: Smalltalk, Pharo, Language Implementation, Research, SOMns, Metrics
  author: Stefan Marr
---


```{r setup, echo=FALSE, include=FALSE, cache=FALSE}
if (!suppressPackageStartupMessages(library(here, logical.return=TRUE))) {
  install.packages("here", repos="https://cloud.r-project.org/")
  library(here)
}
source(here("libs", "common.R"), chdir = TRUE)


knitr::opts_chunk$set(
  echo = FALSE,
  dev = "ragg_png",
  dpi = 192) #  dpi = 192

knitr::knit_hooks$set(
  plot = function(x, options) {
    cap  <- options$fig.cap  # figure caption
    tags <- htmltools::tags
    as.character(tags$img(src = x, alt = cap))
  }
)

# install.packages("ParetoPosStable")
library(ParetoPosStable)

base_dir <- '/Users/smarr/Projects/Smalltalk-Code-Characteristics/data-2020-12/'

classes_files <- c(
  './part-2/internal/classes.csv',
  './part-1/internal/classes.csv'
)

method_files <- c(
  './part-2/internal/methods.csv',
  './part-1/internal/methods.csv'
)

load_all <- function (file_names) {
  file_names %>%
    map(function (file_name) {
      cat(file_name, " ")
      read.csv(paste0(base_dir, file_name))
    }) %>%
    reduce(rbind)
}

unify_projects <- function(input_data) {
  # This makes sure that relate projects are treated as one.
  # Typically these projects don't use the subpackage notation but for our
  # purposes are related nontheless.
  input_data %>%
    mutate(project = ifelse(str_starts(project, "SmaCC"), "SmaCC", project)) %>%
    mutate(project = ifelse(str_starts(project, "Petit"), "Petit", project)) %>%
    mutate(project = ifelse(str_starts(project, "Code"), "Code", project)) %>%
    mutate(project = ifelse(str_starts(project, "Deprecated"), "Deprecated", project)) %>%
    mutate(project = ifelse(str_starts(project, "Enlumineur"), "Enlumineur", project)) %>%
    mutate(project = ifelse(str_starts(project, "Famix"), "Famix", project)) %>%
    mutate(project = ifelse(str_starts(project, "Metacello"), "Metacello", project)) %>%
    mutate(project = ifelse(str_starts(project, "Monticello"), "Monticello", project)) %>%
    mutate(project = ifelse(str_starts(project, "Scripting"), "Scripting", project)) %>%
    mutate(project = ifelse(str_detect(project, "FFI"), "FFI", project))
}

classes_data <- load_all(classes_files) %>%
  select(-X) %>%
  distinct(class, .keep_all = TRUE)
classes_data$isTestClass <- as.logical(classes_data$isTestClass)
classes_data <- classes_data %>%
  unify_projects()

methods_data <- load_all(method_files) %>%
  select(-c(X, id)) %>%
  distinct(class, selector, .keep_all = TRUE)
methods_data$isTestClass <- as.logical(methods_data$isTestClass)
methods_data <- methods_data %>%
  unify_projects()
```

Recently, I was wondering how large code bases look like when it comes to
the basic properties compiler might care about. And here I am not thinking about
dynamic properties, but simply static properties such as length of methods,
number of methods per class, number of fields, and so on.

I think there are a whole bunch of studies that ask questions related to this.
And a quick search let me to a report titled [Characterizing Pharo Code](https://hal.inria.fr/hal-02440055/document) by Zaitsev et al.,
which also comes with the code the authors used to answer their questions.

Though, the report focuses on more high-level questions than what I had in mind.
With [a bit of extra effort](https://github.com/smarr/SourceCodeDataCollector/tree/extra-data), I managed to collect the data I was looking for.

```{r calc-size, cache=TRUE}
num_projects <- length(levels(factor(methods_data$project)))
num_packages <- length(levels(factor(methods_data$package)))

# note that methods_data and classes_data give slightly different results because
# of the extension methods. So, the classes_data is better, because
# it doesn't have unwanted duplication.
num_classes  <- length(levels(factor(classes_data$class)))
num_test_classes <- length(levels(factor((classes_data %>% filter(isTestClass))$class)))

num_methods <- nrow(methods_data)
num_test_methods <- nrow(methods_data %>% filter(isTestClass))
```

## Methodology

The report by Zaitsev et al. selected Pharo projects that represent a variety of
different domains, widely used and less widely used projects,
small and large ones, as well as active and less active projects.
I kept the same selection of projects, but with a slightly more recent
set of commits to look at.

Furthermore, I included the whole Pharo 8.0 base system and all loaded
dependencies, which didn't seem to be the case in the original analysis.

A full list of projects and commits is included at the end of this post.
Overall, the analysis considers `r pretty(num_projects)` projects, with
`r pretty(num_packages)` packages in total.
A "project" is here a set of Pharo packages that are related by name.
This includes for instance [Moose](https://moosetechnology.org/),
a platform for software and data analysis, [Seaside](http://www.seaside.st/),
a web application framework, [Roassal](http://agilevisualization.com/), scripting
for visualizations, and various other packages, including the Pharo system itself.

Since Pharo has the classic introspection/reflection facilities of Smalltalk
systems, I use them to collect the structural metrics, including lines of code,
number of methods, classes, arguments, and local variables.

## Size of the Overall Code Base

As mentioned earlier, the code base under investigation is composed of
`r pretty(num_projects)` projects.
These projects contain `r pretty(num_classes)` classes,
of which `r pretty(num_test_classes)` classes are unit tests. 
Overall, there are `r pretty(num_methods)` methods in the system,
of which `r pretty(num_test_methods)` are on test classes.

This means, about `r pro(num_test_classes/num_classes)`% of the classes
and `r pro(num_test_methods/num_methods)`% of the methods are related to tests.
Since this seems to be a rather small number, I'll keep the test code in
the analysis even so the code may have different general properties.

```{r plot-pkgproj}
num_hist <- function (data, binwidth, fill, ylab, xlab) {
  plot <- ggplot(data, aes(x=num)) +
    geom_histogram(binwidth = binwidth, fill = fill) +
    scale_force_origin_continuous2() +
    xlab(xlab) +
    ylab(ylab) +
    theme_simple(8) +
    theme(plot.margin = unit(c(5,0,0,0), "mm"))
  if (is.null(ylab)) {
    plot <- plot + theme(axis.title.y = element_blank())
  }
  plot
}
```

To get an impression how classes are distributed over packages and projects,
let's look at the following plot.

```{r per-pkgproj-class-nums, results='asis', cache=TRUE, fig.height=2.6, fig.width=8}
num_classes_per_package <- classes_data %>%
    group_by(package) %>%
    summarise(
      num = n(),
      .groups = "drop")
num_classes_per_project <- classes_data %>%
    group_by(project) %>%
    summarise(
      num = n(),
      .groups = "drop")

cp("<figure class='full'>")
grid.arrange(
  num_hist(num_classes_per_package %>% filter(num < 75), 1, "#3465a4", "number of packages", "number of classes"),
  num_hist(num_classes_per_package %>% filter(num >= 75), 25, "#3465a4", NULL, "number of classes"),
  num_hist(num_classes_per_project, 25, "#75507b", "number of projects", "number of classes") +
    theme(plot.margin = unit(c(5,5,0,0), "mm")),
  ncol = 3)
cp("<figcaption>Number of classes per package and per project.</figcaption>")
cp("</figure>")
```

The first two graphs are histograms that show how many packages have a specific number of classes in them.
We only record something if there's a method, and a method needs a class.
So, there are no packages without any classes.
But there are plenty of packages with only 1 class.
The number of packages that have high number of classes decreases rapidly.
The second histogram shows all packages that have 75 or more classes,
and we see there are two packages with around 600 classes: `r str_c((num_classes_per_package %>% filter(num >= 500))$package, collapse = ", ")`.

The third histogram looks at the same data but this time by project.
A project can consist of multiple packages, but it turns out,
there are many projects with very few classes, and only very few projects 
with many classes.
To make these details better visible, the second and third histogram
uses a bin size of 25 instead of 1.

When looking at the following plots, we see that the results look a bit different
for methods.

```{r per-clspkg-method-nums, results='asis', cache=TRUE, fig.height=2.6, fig.width=8}
num_methods_per_package <- methods_data %>%
    group_by(package) %>%
    summarise(
      num = n(),
      .groups = "drop")
num_methods_per_project <- methods_data %>%
    group_by(project) %>%
    summarise(
      num = n(),
      .groups = "drop")

cp("<figure class='full'>")
grid.arrange(
  num_hist(num_methods_per_package %>% filter(num < 100), 1, "#3465a4", "number of packages", "number of methods"),
  num_hist(num_methods_per_package %>% filter(num >= 100), 50, "#3465a4", NULL, "number of methods"),
  num_hist(num_methods_per_project, 100, "#75507b", "number of projects", "number of methods") +
    theme(plot.margin = unit(c(5,5,0,0), "mm")),
  ncol = 3)
cp("<figcaption>Number of methods per package and per project</figcaption>")
cp("</figure>")
```

The first histogram (on the left) shows how many packages have 1, 2, and up to 99 methods
in them. There seem to be about 15 packages with just a single method in them.
And about 40 with 2 methods.
Interestingly, the number of methods per package seems to show fewer
similarities to the power law or pareto distribution
than the number of classes.

Looking at the second histogram, which only considers the packages with
100 or more methods, we see a shape more similar to the power law.

When looking at the data at the granularity of projects, in the third histogram,
we see many projects with very few methods, and only very few projects with many methods.

In this corpus, the projects Bloc, Glamorous Toolkit, SmaCC, and Spec all have more than 10,000 methods.

<!-- TODO: what's the package per project distribution? -->

## Structure of Classes

Let us assume for the rest of this post that this is a single code base.
In Pharo, it would feel like a single code base anyway, since everything is in
the image and can be accessed and modified easily.

```{r per-class-method-nums, results='asis', cache=TRUE, fig.height=2.6, fig.width=5.3}
num_methods_per_class <- methods_data %>%
    group_by(class) %>%
    summarise(
      num = n(),
      .groups = "drop")

cp("<figure class='two-thirds'>")
grid.arrange(
  num_hist(num_methods_per_class %>% filter(num < 50), 1, "#3465a4", "number of classes", "number of methods"),
  num_hist(num_methods_per_class %>% filter(num >= 50), 10, "#3465a4", NULL, "number of methods") +
    theme(plot.margin = unit(c(5,5,0,0), "mm")),
  ncol = 2)
cp("<figcaption>Number of methods per class.</figcaption>")
cp("</figure>")
```

The two histograms above show the number of classes that have a particular number of methods.
On the left, we see all class with fewer than 50 methods.
Turns out, a lot of classes have a single method, and even though there are considerably fewer,
there are quite a number of classes with 40 to 50 methods.
In the histogram on the right, with a bin size of 10,
we see that there are still plenty of classes with 50 to 100 methods,
after which we then find fewer and fewer classes.
The classes Morph, Object, and VBNetParser have each more than 700 methods, and thus,
have the most methods.

```{r per-class-direct-fields-nums, results='asis', cache=TRUE, fig.height=2.6, fig.width=5.3}
num_dfields_per_class <- classes_data %>%
    rename(num = numDirectInstVars)

cp("<figure class='two-thirds'>")
grid.arrange(
  num_hist(num_dfields_per_class %>% filter(num < 15), 1, "#3465a4", "number of classes", "number of direct fields"),
  num_hist(num_dfields_per_class %>% filter(num >= 15), 1, "#3465a4", NULL, "number of fields") +
    theme(plot.margin = unit(c(5,5,0,0), "mm")),
  ncol = 2)
cp("<figcaption>Number of direct fields in a class.</figcaption>")
cp("</figure>")
```

The histograms above show how many classes have a specific number fields
that they directly declare. For comparison below, we'll look at the
total number of fields of a class, considering all fields of its superclasses.

For direct fields, we see that may classes do not have any fields, but plenty of them have some
fields. In the histogram on the right, we see quite a number of classes
with 15 or more fields (`r nrow(num_dfields_per_class %>% filter(num >= 15))` in total).
The classes with more than 100 methods are PRPillarGrammar, PRPillarGrammarOld,
PPYAMLGrammar, and FamixGenerator.

```{r per-class-total-fields-nums, results='asis', cache=TRUE, fig.height=2.6, fig.width=5.3}
num_tfields_per_class <- classes_data %>%
    rename(num = numTotalInstVars)

cp("<figure class='two-thirds'>")
grid.arrange(
  num_hist(num_tfields_per_class %>% filter(num < 30), 1, "#3465a4", "number of classes", "number of all fields"),
  num_hist(num_tfields_per_class %>% filter(num >= 30), 1, "#3465a4", NULL, "number of all fields") +
    theme(plot.margin = unit(c(5,5,0,0), "mm")),
  ncol = 2)
cp("<figcaption>Number of all fields in a class including from superclasses.</figcaption>")
cp("</figure>")
```

When considering all fields, including the ones in the superclass hierarchy,
things look a little different.
On the left, we see the number of classes that have fewer than 30 fields.
Since we now count the classes from the superclass hierarchy,
we see there's a spike at three fields.
For classes with 30 or more fields in total, in the histogram on the right,
we see a few more spikes, but at a smaller level.
The class with the most fields is `r (num_tfields_per_class %>% filter(num == max(num_tfields_per_class$num)))$class` and
has `r max(num_tfields_per_class$num)` fields.


```{r per-class-fields-nums, results='asis', cache=TRUE, fig.height=2.6, fig.width=5.3}
num_cfields_per_class <- classes_data %>%
    rename(num = numClassVars)

cp("<figure class='two-thirds'>")
grid.arrange(
  num_hist(num_cfields_per_class, 1, "#3465a4", "number of classes", "number of class fields"),
  num_hist(num_cfields_per_class %>% filter(num >= 1), 1, "#3465a4", NULL, "number of class fields") +
    theme(plot.margin = unit(c(5,5,0,0), "mm")),
  ncol = 2)
cp("<figcaption>Number of class fields, i.e., static fields per class.</figcaption>")
cp("</figure>")
```

When it comes to class fields, the situation looks very different.
Only `r nrow(num_cfields_per_class %>% filter(num == 1))` classes have one class field,
and only `r nrow(num_cfields_per_class %>% filter(num > 1))` have more than one.


```{r per-class-super-nums, results='asis', cache=TRUE, fig.height=2.6, fig.width=5.3}
num_superclasses_per_class <- classes_data %>%
    rename(num = numSuperclasses)

cp("<figure class='two-thirds'>")
grid.arrange(
  num_hist(num_superclasses_per_class, 1, "#3465a4", "number of classes", "number of superclasses"),
  num_hist(num_superclasses_per_class %>% filter(num >= 8), 1, "#3465a4", NULL, "number of superclasses") +
    theme(plot.margin = unit(c(5,5,0,0), "mm")),
  ncol = 2)
cp("<figcaption>Number of superclasses per class.</figcaption>")
cp("</figure>")
```

Since the number of fields depends on the superclass hierarchy,
let's have a look at the numbers for using inheritance.

The data looks a bit strange. We have few classes that have no superclass.
This is a quirk in Pharo's reflection system.
These classes are not classic classes but traits.
The few classes that have a single superclass are bit special,
and reflect Pharo's metalevel architecture. The most important one is Object.
Its superclass is ProtoObject, where the hierarchy terminates.
The other classes are what can be considered dynamic proxies,
used for intercepting message sends/method calls.

Only few hierarchies turn out to be deep,
which includes widgets and some test classes with 11 or 12 superclasses.

## Structure of Methods

After looking at classes, let's investigate the methods a bit closer.

```{r loc-per-methods, results='asis', cache=TRUE, fig.height=2.6, fig.width=8}
loc_per_method <- methods_data %>%
    rename(num = linesOfCode)

cp("<figure class='full'>")
grid.arrange(
  num_hist(loc_per_method %>% filter(num < 30), 1, "#3465a4", "number of methods", "number of lines"),
  num_hist(loc_per_method %>% filter(num >= 30 & num < 150), 1, "#3465a4", NULL, "number of lines"),
  num_hist(loc_per_method %>% filter(num >= 150), 250, "#3465a4", NULL, "number of lines") +
    theme(plot.margin = unit(c(5,5,0,0), "mm")),
  ncol = 3)
cp("<figcaption>Lines of code per method.</figcaption>")
cp("</figure>")
```
There's indeed one method in a mock class that has no code.
Not sure what's going on there, but the method might simply not be a
source method. I didn't check.
Though, there are `r nrow(loc_per_method %>% filter(num == 1))` methods with one
line of code. First this seemed a little strange, too, but since Pharo
considers method signatures as part of the method, it's essentially empty methods.
With this, it's unsurprising that most methods have 2 lines, which includes
accessors and all kind of other short methods.

If I recall correctly, Smalltalkers advice against methods with more than 6 or 7 lines.
From the data distribution, the advice seems to be widely ignored.
At least, there doesn't seem to be a major step after 6-7 lines.
There are `r nrow(loc_per_method %>% filter(num > 1000))` methods with more 
than 1,000 lines.
The `r nrow(loc_per_method %>% filter(num >= 5000))` methods with more than 5,000
lines seem to all carry various kind of data, things like JSON and JavaScript
strings.

```{r loc-per-class, results='asis', cache=TRUE, fig.height=2.6, fig.width=8}
loc_per_class <- methods_data %>%
  group_by(class) %>%
  summarise(num = sum(linesOfCode),
            .groups = "drop")

cp("<figure class='full'>")
grid.arrange(
  num_hist(loc_per_class %>% filter(num < 100), 1, "#3465a4", "number of classes", "number of lines"),
  num_hist(loc_per_class %>% filter(num >= 100 & num < 1000), 10, "#3465a4", NULL, "number of lines"),
  num_hist(loc_per_class %>% filter(num >= 1000), 250, "#3465a4", NULL, "number of lines") +
    theme(plot.margin = unit(c(5,5,0,0), "mm")),
  ncol = 3)
cp("<figcaption>Lines of code per class.</figcaption>")
cp("</figure>")
```

Looking at the lines of code by aggregating them per class reveals a mostly similar picture.
Many tiny classes, and few large classes.


```{r args-per-method, results='asis', cache=TRUE, fig.height=2.6, fig.width=8}
args_per_method <- methods_data %>%
    rename(num = numArgs)

cp("<figure class='full'>")
grid.arrange(
  num_hist(args_per_method %>% filter(num < 15), 1, "#3465a4", "number of methods", "number of arguments"),
  num_hist(args_per_method %>% filter(num >= 4), 1, "#3465a4", NULL, "number of arguments"),
  num_hist(args_per_method %>% filter(num >= 7), 1, "#3465a4", NULL, "number of arguments") +
    theme(plot.margin = unit(c(5,5,0,0), "mm")),
  ncol = 3)
cp("<figcaption>Number of arguments per method.</figcaption>")
cp("</figure>")
```

When looking at the number of arguments a method takes, we see a huge number not taking any at all (the receiver is not considered).
About half of the methods has 1 argument, which seems plausible considering setters have
one argument.
The two methods with 15 arguments are methods to test the bytecode compiler.


```{r locals-per-method, results='asis', cache=TRUE, fig.height=2.6, fig.width=8}
locals_per_method <- methods_data %>%
    rename(num = numLocals)

cp("<figure class='full'>")
grid.arrange(
  num_hist(locals_per_method %>% filter(num < 15), 1, "#3465a4", "number of methods", "number of locals"),
  num_hist(locals_per_method %>% filter(num >= 4), 1, "#3465a4", NULL, "number of locals"),
  num_hist(locals_per_method %>% filter(num >= 10), 1, "#3465a4", NULL, "number of locals") +
    theme(plot.margin = unit(c(5,5,0,0), "mm")),
  ncol = 3)
cp("<figcaption>Number of local variables in a method.</figcaption>")
cp("</figure>")
```

A lot of methods don't have any local variables. Probably not surprising
given the number of getters and setters why may assume.
And, it seems people don't actually go all out when it comes to local variables.
`r max(locals_per_method$num)` variables seem sufficient for everyone,
and the particular method seems to rotate an elliptical arc,
thus, implements a somewhat complex algorithm.


```{r literals-per-method, results='asis', cache=TRUE, fig.height=2.6, fig.width=5.3}
literals_per_method <- methods_data %>%
    rename(num = numLiterals)

cp("<figure class='two-thirds'>")
grid.arrange(
  num_hist(literals_per_method %>% filter(num < 50), 1, "#3465a4", "number of methods", "number of literals"),
  num_hist(literals_per_method %>% filter(num >= 50), 1, "#3465a4", NULL, "number of literals") +
    theme(plot.margin = unit(c(5,5,0,0), "mm")),
  ncol = 2)
cp("<figcaption>Number of literals in a method.</figcaption>")
cp("</figure>")
```

Finally, let's have a look at the number of literals per method.
Literals include any kind of numbers, constants, and constructs that have a
specific syntax in the language, e.g., arrays.
However, it also includes the names of methods, which are used for the message
sends. Thus, it somewhat correlates with the number of message sends a
method may possibly have. Though, that's probably not a perfect
correlation because of the other kinds of literals as well as
all kind of optimizations in the bytecode set.


## Conclusion

Ok, so, what to do with this data?
I am not quite sure yet.
Though, there are a few bits and pieces in here that are interesting.
And, since I recently started generating large code bases to
assess the performance of cold code, i.e., interpreter speed,
I think some of these bits will allow me to generate more "natural" code.

Other details suggest to have a good look at various optimizations classic interpreters do.
For example, SOMns optimizes the accessor methods to object fields already, and thus avoids 
a full method/function call for them.
Not sure whether that's an optimization applied by many languages,
though, HotSpot does it under the term "fast accessor methods".

Would also be interesting to see how these numbers compare across
languages. Python and Ruby come to mind as similar class-based dynamic languages.

There might be more to gain from this data, but that's for another day.

For suggestions, comments, or questions, find me on Twitter [@smarr](https://twitter.com/smarr).

```{r}
### Distribution of Method Length
# fit <- PPS.fit((methods_short %>% filter(linesOfCode > 0))$linesOfCode)
# plot(fit)

pfit <- pareto.fit((methods_data %>% filter(linesOfCode > 0))$linesOfCode)
#  #sigma is scale, scale is b
plot(pfit)

### Attempting to generate Pareto distributed random numbers

# gen_nums <- runif(length(methods_short$linesOfCode))
# a <- pfit$estimate$lambda
# b <- pfit$estimate$sigma
# 
# inv_fun_denom <- (1 - gen_nums) ^ (1/a)
# pareto_nums <- (b / inv_fun_denom) - b
# 
# pareto_data <- data.frame(linesOfCode = pareto_nums)
# 
# ggplot(pareto_data, aes(x=linesOfCode)) +
#   geom_histogram()
# 
# pfit$estimate$lambda
# pfit$estimate$sigma

```



## Appendix

The following table contains the details on the projects included in this
analysis.

| Project     | Commit  | URL |
|-------------|---------|-----|
| DrTests     | 010eb9b | https://github.com/juliendelplanque/DrTests |
| Mustache    | 728feda | https://github.com/noha/mustache |
| PetitParser | bd108b9 | https://github.com/moosetechnology/PetitParser |
| Pillar      | 4d8a285 | https://github.com/pillar-markup/pillar        |
| Seaside     | e0c73a5 | https://github.com/SeasideSt/Seaside |
| Spec2       | 988c6d7 | https://github.com/pharo-spec/Spec |
| PolyMath    | 473b0b0 | https://github.com/PolyMathOrg/PolyMath |
| Telescope   | 8c47cfc | https://github.com/TelescopeSt/TelescopeCytoscape |
| Voyage      | f4f9d28 | https://github.com/pharo-nosql/voyage |
| Bloc        | a8c7ecb | https://github.com/pharo-graphics/Bloc |
| DataFrame   | 7422404 | https://github.com/PolyMathOrg/DataFrame |
| Roassal2    | d65a87a | https://github.com/ObjectProfile/Roassal2 |
| Roassal3    | 167de2d | https://github.com/ObjectProfile/Roassal3 |
| Moose       | fc8fb07 | https://github.com/moosetechnology/Moose |
| GToolkit    | e3c98fc | https://github.com/feenkcom/gtoolkit |
| Iceberg     | 7e78a75 | https://github.com/pharo-vcs/iceberg |

```{r}
# After loading these projects into a Pharo 8.0 image,
# the following `r length(levels(factor(methods_data$project)))`
# projects were loaded: `r str_c(levels(factor(methods_data$project)), collapse = ", ")`
```

```{r}

bins <- seq(0, max(loc))
# bins <- 0:31
loc <- methods_data$linesOfCode
bins_data <- sapply(bins, function(i) {
  length(loc[loc == i])
})

df <- data.frame(bins = bins, num = bins_data) %>%
  filter(num > 0)
ggplot(df, aes(x = bins, y = num)) +
  geom_point() +
  scale_y_log10() +
  scale_x_log10() +
  theme_simple(8) +
  ylab("number of methods") +
  xlab("lines of code") +
  theme(plot.margin = unit(c(5,0,0,0), "mm"))


my_fitted_fn <- function(x) {
  -0.00003818 * exp(3.63181043 * x)
}

```
```{r}
my_fitted_fn <- function(x) {
  #1000000 * exp(x / -5) / x^3
  #500000 / x^2 * exp(-x/27)
  # 800000 / x^2.5 * exp(-x/30)
  #773905.529245 / x^2.7 * exp(-x/100)
  #94338.9316 / x^0.6343 * exp(-x/7.5584)
  92695.9275831 / x^0.6132192 * exp(x / -7.3835094)
}
fitted <- my_fitted_fn(bins)
df <- data.frame(bins = bins, num = bins_data, fitted = fitted) %>%
    filter(num > 10 & bins > 1 & num > 30 & bins <= 30)
ggplot(df, aes(x = bins)) +
  geom_point(aes(y = num), color="blue") +
  geom_point(aes(y = fitted), color="orange") +
  scale_y_log10() + scale_x_log10() + 
  ylab("number of methods") + xlab("lines of code") + theme_simple(8) + theme(plot.margin = unit(c(5,0,0,0), "mm"))

# View(fitted)
```

```{r}
start <- list(a = 800000, b = 2.5, c=-30)
model <- nls(num ~ a / bins^b * exp(bins / c), data=df, start=start, trace=TRUE,
             control=nls.control(
               minFactor = 1/(1024 * 1024 * 1024),
               maxiter = 1000))
library(formula.tools)
invert(num ~ a / bins^b * exp(bins / c))
```


```{r}
n <- nrow(methods_data)
numbers <- runif(n)

df <- data.frame(num = numbers)
ggplot(df, aes(x=num)) + geom_histogram() + geom_density()

# double legacy_standard_exponential(aug_bitgen_t *aug_state) {
#   /* We use -log(1-U) since U is [0, 1) */
#   return -log(1.0 - legacy_double(aug_state));
# }
# 
# double legacy_pareto(aug_bitgen_t *aug_state, double a) {
#   return exp(legacy_standard_exponential(aug_state) / a) - 1;
# }

std_exp <- function(U) {
  #-log(1.0 - U)
  -log1p(-U)
}

std_exp_num <- std_exp(numbers)
df <- data.frame(num = std_exp_num)
ggplot(df, aes(x=num)) + geom_histogram() + geom_density()

pareto <- function (U, a) {
  # exp(std_exp(U) / a) - 1
  expm1(std_exp(U) / a)
}

pareto_num <- expm1(pareto(numbers, 50)) * 50000
df <- data.frame(num = pareto_num)
grid.arrange(
  ggplot(df, aes(x=num)) + geom_histogram(binwidth = 10) + geom_density(),
  ggplot(methods_data, aes(x=linesOfCode)) + geom_histogram(binwidth = 10) + geom_density(),
  ncol = 2
)
```

## TODO:
- manually determine the numbers of methods with 0-30 LOC

```{r}
bins <- 0:30
# bins <- 0:31
loc <- methods_data$linesOfCode
bins_data <- sapply(bins, function(i) {
  if (i < 31) {
    length(loc[loc == i])
  } else {
    sum(loc[loc >= 30]) / 30
  }
})

ggplot(data.frame(bins = bins, num = bins_data), aes(x = bins, y = num)) +
  # geom_col()
  geom_point() +
  scale_y_log10() +
  scale_x_log10()
  


total <- sum(bins_data)
parts <- bins_data / total
ggplot(data.frame(bins = factor(bins), num = parts), aes(x = bins, y = num)) +
  # geom_col()
  geom_point()


prefix_sum_parts <- cumsum(parts)
ggplot(data.frame(bins = factor(bins), num = cumsum(parts)), aes(x = bins, y = num)) + geom_col()
sum(parts)


loc_from_u <- function (u) {
  Position(function (e) { u < e }, prefix_sum_parts)
}
 
random_data <- runif(length(methods_data$linesOfCode))
loc_data <- sapply(random_data, loc_from_u)
ggplot(data.frame(num = loc_data),
       aes(x = num)) +
  geom_histogram(binwidth = 1)
sum(loc_data)

```
squeeze all methods into 0-30LOC methods
see what distribution looks like.
determine the ratios for each of the 0-30 segements.
Uses that to split the uniform range and distribute random numbers
